---
title: "STAT/MATH 495: Advanced Data Analysis"
author: "Albert Y. Kim"
date: "Last updated on `r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 1
    toc_float: true
---

<style>
h1{font-weight: 400;}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
library(tidyverse)
library(broom)
library(knitr)
library(modelr)
library(lubridate)
library(forcats)
```

```{r, eval=FALSE, echo=FALSE}
# Run this separately to have slide output:
rmarkdown::render("index.Rmd", output_format = c("ioslides_presentation"), output_file = "slides.html")
```


```{r, eval=FALSE, echo=FALSE}
# Code to randomly assign students to groups:
library(tidyverse)
class <- c("Brenna", "Anthony", "Abbas", "Sara", "Sarah", "Jonathan", "Harrison", "Tasheena", "Leonard", "Timothy", "Jenn", "Kiryu", "Jeff", "Vickie", "Pei", "Luke", "Brendan", "Caleb", "Meron", "Christien", "Wayne", "Meredith")
set.seed(76)
sample(class) %>% matrix(nrow=2) %>% tbl_df() %>% knitr::kable()
```




***



The lectures are numbered according to the following rough topic outline below. A detailed topic schedule and corresponding readings can be found [here](https://docs.google.com/spreadsheets/d/e/2PACX-1vSoT25RqvzJrr563nky-ANl_EiM39NruG2fkH8fykFxMwk8JXILo5AbdEKtJabcBQ2QM41ab4MDe7Ab/pubhtml?gid=1289440669&single=true).

1. Background
    * ~~Intro to modeling~~ 
    * ~~Simple case with univariate predictors to start: splines~~ Done
    * Sampling/resampling, out-of-sample prediction, crossvalidation
    * Bias-variance tradeoff
1. Continuous outcomes I
    * LOESS smoother
    * Regression for prediction
1. Categorical outcomes i.e. classification
    * Logistic regression for prediction + ROC curves
    * k-Nearest Neighbors
    * Classification and regression trees (CART)
1. Continuous outcome II
    * Regularization/shrinkage methods: Ridge regression and LASSO
1. Other methods
    * Boosting and bagging
    * Random forests
    * Neural nets
1. Unsupervised learning (time permitting)
    * Principal components analysis
    * k-Means Clustering



***


<!--

# 1.8 Crossvalidation 2

## Thu 9/21

Announcements:

* Problem set 02 feedback and example solutions given [below](#PS02).


## Chalk talk

1.8




***



# 1.7 Crossvalidation

## Tue 9/19

Announcements:

* Problem set 02 presentations listed [below](#PS02).


***


# Problem set 3 {#PS03}

## Information

*Assigned on Thu 9/19, due on Tue 9/26 9am at which point there will be synchronized pull requests.*

You will

* Teams: 
* Deliverables: One ready to submit *pull request* to the [PS03 repo](https://github.com/2017-09-Amherst-STAT495/PS03) (recall the GitHub [submission process](https://github.com/2017-09-Amherst-STAT495/README#problem-set-submission-process)). At the very least, the following files should be modified/added:
    1. `PS03.Rmd`: This should
    1. `PS03.html`: The output of `PS03.Rmd`. This will be the presentation file for a randomly chosen set of groups.
    1. `README.md`: Change `rudeboybert` in the URL to the GitHub ID of the team leader so that you can open your team's version of `PS03.html` in a browser on Tuesday morning.
* Tips:
* Hints:


## Presentations



## Feedback



***
-->



# 1.6 Resampling

## Mon 9/18

Announcements:



## Chalk talk

1.6 Resampling



***




# 1.5 The theory of splines

## Thu 9/14

Announcements:

* "Gah! I can't tell who you are from your Slack names!" Please go to: "STAT 495 Amherst..." on top left of Slack -> "Preferences" -> "Messages & Media" -> "Names" -> Click the "Full & display names" radio button.
* Problem set 01 feedback given [below](#PS01).


## Splines

Splines are piecewise continuous polynomials with [smoothness](https://en.wikipedia.org/wiki/Smoothness) constraints:

<center><img src="images/splines.png" alt="Drawing" style="width: 500px;"/></center>


## Chalk talk

1.5 Splines


## How are splines curves fit?

How to obtain the fitted $\widehat{\beta}_0$, $\widehat{\beta}_1$, $\widehat{\beta}_2$, and $\widehat{\beta}_3$ necessary for all $K+1$ cubic polynomials 
$$
\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_i + \widehat{\beta}_2 x_i^2 + \widehat{\beta}_3 x_i^3
$$

for each interval? A combination of calculus (derivatives) and linear algebra (solving a system of equations). See this <a target="_blank" class="page-link" href="static/splines_notes.pdf">PDF</a> if you're curious.


## Sampling paradigm

<center><img src="images/target-population.jpg" alt="Drawing" style="width: 500px;"/></center>

## Chalk talk

1.5 Sampling


## Resampling paradigm

<center><img src="images/resample.jpg" alt="Drawing" style="width: 500px;"/></center>

**Moral**: You want the

* the process represented by the red dashed line/arrow (resampling) to mimic as much as possible
* the process represented by the black dashed line/arrow (sampling)



***



# Problem set 2 {#PS02}

## Information

*Assigned on Thu 9/12, due on Tue 9/19 9am at which point there will be synchronized pull requests.*

<a href="https://www.kaggle.com/c/sberbank-russian-housing-market">
<img border="0" alt="Sberbank" src="images/sberbank.png" width="600">
</a>

You will enter the [Sberbank Russian Housing Market](https://www.kaggle.com/c/sberbank-russian-housing-market) Kaggle competition and fit a spline model to predict the outcome variable `price_doc`: the sale price of a piece of real estate. The data has been pre-downloaded and included in the [PS02 repo](https://github.com/2017-09-Amherst-STAT495/PS02) in the course GitHub Organization.

* Teams: 2-3 people at your choosing. Once you've selected your group designate one person as team leader, who will:
    + Create a Slack DM that includes Albert, Andrew Kim (TA), and all team members. So far:
        + Team A: Sara (no "h"), Meredith, Brenna
        + Team B: Jeff, Luke, Andrew
        + Team C: Jonathan, Sarah (with "h"), Timothy
        + Team D: Pei, Jenn, Anthony
        + Team E: Leonard, Vickie, Brendan
        + Team F: Abbas, Caleb, Kiryu
        + Team G: Harrison, Christien
    + Submit a single pull request on behalf of the group.
    + Submit a single submission to Kaggle on behalf of the group.
* Deliverables: One ready to submit *pull request* to the [PS02 repo](https://github.com/2017-09-Amherst-STAT495/PS02) (recall the GitHub [submission process](https://github.com/2017-09-Amherst-STAT495/README#problem-set-submission-process)). At the very least, the following files should be modified/added:
    1. `PS02.Rmd`: This should
        + Be well-commented and completely reproducible.
        + Involve an *exploratory data analysis*.
        + Argue why you chose the model you did.
    1. `PS02.html`: The output of `PS02.Rmd`. This will be the presentation file for a randomly chosen set of groups on Tue 9/19.
    1. `README.md`: Change `rudeboybert` in the URL to the GitHub ID of the team leader so that you can open your team's version of `PS02.html` in a browser on Tuesday morning.
    1. `submission.csv`
    1. `screen_shot.png` or `screen_shot.jpg`: A screenshot of your Kaggle ranking.
* Tips:
    + Again, do not worry about your score. This problem set is more about process more than outcome.
    + Do not [spin your wheels](https://youtu.be/VrS14wLmVhY?t=12s)!
* Hints:

```{r, fig.height=3, fig.width=5}
library(tidyverse)
library(broom)
data(cars)
splines_model <- smooth.spline(x=cars$speed, y=cars$dist, df = 6)
splines_model_tidy <- splines_model %>% 
  broom::augment() 
plot <- ggplot(splines_model_tidy, aes(x=x)) +
  geom_point(aes(y=y)) +
  geom_line(aes(y=.fitted), col="blue")
plot
new_speeds <- c(24.6, 26.4, 23.4, 16.8, 15.8, 5.4, 14.8, 8.2, 1, 18.8, 25, 28, 19.6, 11.2, 21.8, 26.2, 20.6, 0, 14.4, 8.8)
# What do you think output is?
output <- predict(splines_model, new_data = new_speeds) %>% 
  tibble::as.tibble()
head(output)
plot +
  geom_point(data=output, aes(x=x, y=y), col="red")
```

## Presentations 


## Feedback


## Example solutions


***



# 1.4 Intro to splines

## Tue 9/12

Announcements: New office hours location: Seeley Mudd 208 (lounge)

## Chalk talk

* Analogy of what we are doing


## Splines

Splines are *piecewise cubic polynomials with smoothness constraints*. The code
corresponding to the video at the end of [Lecture
1.3](https://rudeboybert.github.io/STAT495/#13_intro_to_modeling) is below. (If
you're curious, the code that created `multiple_df`, `exercise`, `fitted`, and
`truth` are available on lines 1-86 of <https://bit.ly/rudeboybert_splines>;
these were run behind the scences to keep the code below simple.)

```{r, eval=FALSE}
library(tidyverse)
library(broom)
# Load some pre-computed data
load(url("https://rudeboybert.github.io/STAT495/static/splines.RData"))

# Here is a wacky function f(x)
f <- function(x){
  f_x <- 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
  return(f_x)
}

# For 500 equally spaced values of x between 0 & 1, let's compute and plot f(x) in red.
# Recall that f(x) is the systematic component, or "the signal"
values <- data_frame(
  x = seq(from=0, to=1, length=500),
  f_x = f(x)
)
values %>% 
  ggplot(aes(x=x)) +
  stat_function(fun = f, col="red", size=1)

# We now add the unsystematic error component epsilon to f(x) i.e. the noise, to
# obtain our y's, and hence our observed points in black (x, y)
values <- values %>% 
  mutate(
    epsilon = rnorm(500, 0, sd = 2),
    y = f_x + epsilon
  )
values %>% 
  ggplot(aes(x=x)) +
  stat_function(fun = f, col="red", size=1) +
  geom_point(aes(y=y))

# But remember in real life, we won't know the red curve! If we did, then why
# are we doing any of this? All we observe are the black points. Let's "pretend"
# like we don't know what the red curve is!
values %>% 
  ggplot(aes(x=x)) +
  geom_point(aes(y=y))

# We now fit a 'smoothing spline'. Think of it as a piece of string with a
# specified amount of flexibility, where the flexibility is controlled by the
# "degrees of freedom" df. This blue curve is a "guess/estimate" of the red
# curve f(x), which recall, we are pretending we don't know. Also observe how we
# use the broom::augment() function to convert the output of smooth.spline to
# tidy data frame format.
smooth.spline(values$x, values$y, df=5) %>%
  broom::augment() %>% 
  ggplot(aes(x=x)) +
  geom_point(aes(y=y)) +
  geom_line(aes(y=.fitted), col="blue", size=1)
  
# Play around with the df argument in smooth.spline() above.

# Now let's compare smoothing splines using four different values of the degrees
# of freedom in a plot I precomputed. Which do you think is best? 
multiple_df

# I would say that df=10 roughly is best. df=2 is not quite flexible enough,
# where as df=50 seems to no longer fitting to signal (the true function f) and
# is now fitting to noise. In other words, it is overfitting to this particular
# data set.
multiple_df + 
  stat_function(fun = f, col="red", size=1)

# Exercise. Here are a set of points from a different f(x) and epsilon. With
# your finger trace what you think the true f(x) function looks like. In other
# words, separate the signal from the noise!"
exercise

# Let's fit a spline with 25 degrees of freedom. How close is this to the truth?
fitted

# Ready? Here is the truth! How close were you? Note the noise is normal with mean 0 
# and sd = 12!
truth
```



***



# 1.3 Intro to modeling

## Mon 9/11

Announcements:

* Math/stat table for lunch 12-1:30 Terrace Room A, Valentine
* Syllabus has been finalized, so please read it:
    + Midterm dates
    + Andrew's tutoring hours: Mondays 6pm-8pm in Seeley Mudd 205 starting today
    + All problem sets assigned/due/presented on Tuesdays
* Perform pull request for PS01 *as a class*
* For tomorrow, watch 9m49s video linked below.

Topics:

* Supervised vs unsupervised learning
* Modeling for supervised learning
* (If time) Sampling and resampling


## Chalk talk

* 1.3 Supervised vs unsupervised


## Example of unsupervised learning

Colin Woodard's idea of 11 Nations of North America:

<img src="http://emerald.tufts.edu/alumni/magazine/fall2013/images/features/upinarms-map-large.jpg" alt="Drawing" style="width: 600px;"/>


## Chalk talk

* 1.3 Model for supervised learning


## POTUS 45 on models

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Teaching my students to separate systematic model component from irreducible error component when fitting for outcome variable <a href="https://t.co/7jzuqEHTdp">pic.twitter.com/7jzuqEHTdp</a></p>&mdash; Albert Y. Kim (@rudeboybert) <a href="https://twitter.com/rudeboybert/status/834395069444661249">February 22, 2017</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>


## Chalk talk

* 1.3 Income example
* 1.3 Most common example in Amherst stats courses

## Next steps

* Early on we keep things simple: univariate $\vec{x}$ so that we visualize easily
* For tomorrow (Tuesday), watch the following 9m49s video:

<iframe width="560" height="315" src="https://www.youtube.com/embed/bESJ81dyYro" frameborder="0" allowfullscreen></iframe>



***



# Problem set 1 {#PS01}

## Information

*Assigned on Thu 9/7, due on Mon 9/11 9am.*

<img src="https://www.kaggle.com/content/v/6e70930bb5cf/kaggle/img/logos/kaggle-logo-gray-300.png" alt="Drawing" style="width: 250px;"/>

Baby's first Kaggle competition and GitHub pull request! Specifically the [Titanic: Machine Learning from Disaster](<https://www.kaggle.com/c/titanic>) competition. You will be submitting predictions on who survived/died amongst the passengers randomly assigned to the *test set* and viewing your Kaggle leaderboard score.

* Teams: Individual
* Deliverables: A ready to submit *pull request* to the [PS01 repo](https://github.com/2017-09-Amherst-STAT495/PS01), in other words complete only [steps 1-5](https://github.com/2017-09-Amherst-STAT495/README#problem-set-submission-process) of the problem set submission process. However, you will all be submitting/making the pull request (step 6) at the same time as a group in lecture. The following files should be modified/added from the original
    1. `PS01.Rmd`: This should be well-commented and **completely reproducible**. In other words, if someone else takes this repo, they should be able to reproduce your work with one click of the mouse. This necessitates taking an empathetic view of other users.
    1. `PS01.html`: The output of `PS01.Rmd`
    1. `submission.csv`: Your predictions of who survives based on something other than sex. Current set to match `gender_submissions.csv` on Kaggle.
    1. `screen_shot.png` or `screen_shot.jpg`: A screenshot of your Kaggle ranking. Be sure to "clean" your browsers.
    1. Any other necessary files.
* Tips:
    + This problem set is only about getting used to the process, not about anything substantive. 
    + You don't have to fit actual model for now, but feel free to if you want. Don't even worry about how good your predictions are, just focus on getting a score.
    + Do not [spin your wheels](https://youtu.be/VrS14wLmVhY?t=12s)! If you are stuck, take a breather, and consult others.
* **Note added 9/7 1:30pm**: There is an error in `PS01.Rmd`; replace line 4 of `PS01.Rmd` so that it reads:

```
date: "2017-09-07"
```

## Feedback

* Andrew K. (TA) gave [feedback on PS01 on GitHub](https://github.com/2017-09-Amherst-STAT495/PS01/pulls) and will do so for every problem set by Tuesday evening.
    + You should've received an email.
    + Best viewed in the GitHub webpage GUI in split diff mode.
* Overall comments:
    + `View()` doesn't work in an `.Rmd` file; do not include these.
    + Don't comment for your own sake, but rather comment to be empathetic to your collaborators. In particular your [most important collaborator](http://rmhogervorst.nl/cleancode/blog/2016/05/26/your-most-valuable-collaborator-future-you.html).
    + The [tone of your interactions](http://opensourcesurvey.org/2017/) matters, especially in an open source setting. This take practice.



***



# 1.2 Getting used to infrastructure

## Thu 9/7

Announcements:

* Refresher on Slack notifications
* GitHub organization for the course: Where problem sets will be distributed/submitted
    + Ensure you can access the GitHub organization by clicking "GitHub Org" above.
    + If BOTH your full name and/or a recent picture are not visible [here](https://github.com/orgs/2017-09-Amherst-STAT495/people), please update your GitHub profile.

Topics:

* GitHub pull request theory
* Kaggle competition

## GitHub pull request theory

* Open the following in two separate tabs
    + [Problem set 1](https://github.com/2017-09-Amherst-STAT495/PS01)
    + [Problem set submissions process](https://github.com/2017-09-Amherst-STAT495/README#problem-set-submission-process) 
* Chalk talk: 1.2 GitHub pull requests

## Kaggle data

* Run the code block below to load/inspect the [Titanic data](https://www.kaggle.com/c/titanic/data) from Kaggle (which I've uploaded to the course webpage)
* Chalk talk: 1.2 Kaggle data

```{r, eval=FALSE}
library(tidyverse)
train <- read_csv("https://rudeboybert.github.io/STAT495/static/Titanic/train.csv")
test <- read_csv("https://rudeboybert.github.io/STAT495/static/Titanic/test.csv")
submission <- read_csv("https://rudeboybert.github.io/STAT495/static/Titanic/gender_submission.csv")

glimpse(train)
glimpse(test)
glimpse(submission)

View(train)
View(test)
View(submission)
```

## Who is predicted to survive?

The predictions are based on the sex of the passengers.

```{r, eval=FALSE}
test %>% 
  left_join(submission, by="PassengerId") %>% 
  select(PassengerId, Sex, Survived)
```

## Tech time

* Questions:
    1. What is the "scoring" mechanism for this Kaggle competition?
    1. What score does the above "model" yield for the `train`ing data?
* Start problem set 1




***



# Problem set 0

## Information

*Assigned on Tue 9/5, due on Thu 9/7 lecture time*.

* Set up Git/GitHub to interface with RStudio on your computer by reading and following along to
Chapters 1-15 of ["Happy Git and GitHub for the
useR"](http://happygitwithr.com/)
* If you start spinning your wheels, don't panic! I'll have office hours on
Thursday 2-5pm to help out if you are stuck.



***



# 1.1 Syllabus and background

## Tue 9/5

* Albert's background
* Go over syllabus
* Setting up infrastructure
    1. Intro to Slack
    1. Intro to Kaggle
    1. Intro to GitHub


## Your instructor

<http://rudeboybert.rbind.io/>



## What is machine learning?

</br>
<center><img src="images/data_science_3.png" alt="Drawing" style="width: 300px;"/></center>
</br>


## My philosophy

> * **Question** What is the difference between statistics, data science, and machine learning?
> * **Answer** Ben Baumer at Smith posed: "Instead of obsessing over Venn diagrams of what topics are within the domains of which disciplines, I ask instead": What if we blew up math, stats, CS, and all their legacies and started over? What would this field look like/be called?"


## Definitions

* Arthur Samuel (1959): Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed.
* Albert Y. Kim (2017): Prediction. Examples:
    + [Self-Driving Vehicles](https://www.wired.com/2016/10/ubers-self-driving-truck-makes-first-delivery-50000-beers/)
    + [Netflix](https://www.netflix.com/browse) recommendations
    + Simple linear regression *for prediction* and *not for explanation*


## Syllabus

[Syllabus](syllabus.html) discussion.


## Toolbox: `tidyverse`

<img src="images/tidyverse.png" alt="Drawing" style="width: 800px;"/>

    
## Setting up infrastructure

1. Intro to Slack
1. Intro to Kaggle
1. Intro to GitHub


## Intro to Slack

* Slack is a medium for communication that has many pros (and some cons)
* I require you to use Slack via the [Desktop App](https://slack.com/downloads/)
* Student [feedback](https://docs.google.com/spreadsheets/d/1UZmhutoc17ZoWqf6jX9-m_8Ewvw6amVu__X0MMqlbEU)


## Tech time

* Make sure you've completed the [intro survey](https://docs.google.com/forms/d/17tNYB4VqEfXvxvNvIhVvb5OmsTldXQ19OdCYUyYzvl0/edit)
* Install the Slack [Desktop App](https://slack.com/downloads/)
* GitHub
    + If you don't have a [GitHub.com](https://github.com/) account already, create one.
    + Update your name and post a *recent* profile picture.
* Take a break


## Slack

<!--
1. Slack team
1. Slack channel
1. Direct messages: individual, group, to yourself
1. Mentions
1. Sharing code:
    1. in-line using `
    1. Code blocks using ``` and shift+enter
1. File drag and drop
1. Edit profile
-->

* Chalk talk: 1.1 Slack
* Exercise: 
    + Send me your GitHub login via direct message (DM)
    + Practice sending group DM with seatmates
* Key for Slack success: Setting notifications


## Intro to Kaggle

* Baby's first Kaggle competition: [Titanic](https://www.kaggle.com/c/titanic)
* Chalk talk: 1.1 Kaggle


## Intro to GitHub

* My GitHub profile: [`rudeboybert`](https://github.com/rudeboybert)
* Example: `ggplot2` [source code](https://github.com/tidyverse/ggplot2)











