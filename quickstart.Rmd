---
title: "Quickstart Guide to Models"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
---

<style>
h1{font-weight: 400;}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
set.seed(76)
# knitr::purl("quickstart.Rmd", output = "quickstart.R", documentation=1)
```



# Regularization

```{r}
library(tidyverse)
credit <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Credit.csv") %>%
  mutate(ID=1:n()) %>% 
  select(ID, everything())
```

## LASSO

### Fit/train model

```{r}
# Load packages and wrapper functions. Note understanding the internal workings
# of the wrapper functions is not required. If you're curious why all this is
# needed: https://github.com/tidyverse/broom/issues/226
library(glmnet)
get_LASSO_coefficients <- function(LASSO_fit){
  coeff_values <- LASSO_fit %>% 
    broom::tidy() %>% 
    as_tibble() %>% 
    select(-c(step, dev.ratio)) %>% 
    tidyr::complete(lambda, nesting(term), fill = list(estimate = 0)) %>% 
    arrange(desc(lambda)) %>% 
    select(term, estimate, lambda)
  return(coeff_values)
}

# Define model formula:
model_formula <- as.formula("Balance ~ Income + Limit + Rating + Student + Cards + Age + Education + Married")

# Define "model matrix": Note that
# -This function conveniently converts all categorical outcomes to numerical
# ones using "one-hot encoding" as defined in the flashcard for Lec 3.7.
# -We also remove the first column corresponding to the intercept
predictor_matrix <- model.matrix(model_formula, data = credit)[, -1]

# Define values of tuning/complexity parameter lambda. Note:
# -They increase at an exponential rate in powers of 10
lambda_inputs <- 10^seq(-2, 10, length = 100)

# Fit the model using glmnet. Note:
# -Setting alpha=1 corresponds to LASSO.
# -Here we manually specify the lambda values to use. If we didn't set 
# lambda = lambda_inputs, glmnet() would choose default values.
LASSO_fit <- glmnet(x=predictor_matrix, y=credit$Balance, alpha = 1, lambda = lambda_inputs)
```


### Coefficients analysis

For each predictor and each value of lambda_inputs, let's look at the $\widehat{\beta}$:

```{r}
LASSO_coefficients <- get_LASSO_coefficients(LASSO_fit)
ggplot(LASSO_coefficients, aes(x=lambda, y=estimate, col=term)) +
  geom_line() +
  labs(x="lambda", y="beta-hat estimate")
```

There are two problems with this plot:

1. We're not interested in the value of the intercept, since this is left out
of complexity penalization. So let's ignore this.
1. The values of $\lambda$ on the x-axis are difficult to see, so let's rescale
the x-axis to be on a log-10 scale:

```{r}
plot_LASSO_coefficients <- LASSO_coefficients %>% 
  filter(term != "(Intercept)") %>% 
  ggplot(aes(x=lambda, y=estimate, col=term)) +
  geom_line() +
  scale_x_log10() +
  labs(x="lambda (log10-scale)", y="beta-hat estimate")
plot_LASSO_coefficients
```


### Crossvalidation

Easy! Just add `cv.` to previous `glmnet()` call. Also, let's pull out the

* Value of $\lambda$ corresponding to the minimal MSE
* Value of $\lambda$ corresponding to the simplest model within one standard error of minimal MSE

```{r}
LASSO_CV <- cv.glmnet(x=predictor_matrix, y=credit$Balance, alpha = 1, lambda = lambda_inputs)

# Optimal lambdas
lambda_optimal <- LASSO_CV$lambda.min
lambda_optimal_one_SE <- LASSO_CV$lambda.1se
```

Let's plot the result in base R. Note that we are plotting $\log(\lambda)$ on x-axis instead of $\lambda$ on a $\log10$-scale like above.

```{r}
plot(LASSO_CV)
abline(v=log(lambda_optimal), col="red")
abline(v=log(lambda_optimal_one_SE), col="blue")
```

What does this mean in terms of the values of the $\widehat{beta}$'s?

```{r}
plot_LASSO_coefficients <- plot_LASSO_coefficients +
  geom_vline(xintercept = lambda_optimal, col="red", alpha=0.4, linetype="dashed") +
  geom_vline(xintercept = lambda_optimal_one_SE, col="blue", alpha=0.4, linetype="dashed")
plot_LASSO_coefficients
```




### Predict outcomes for test data

Let's use the value of $\lambda$ corresponding to the simplest model within one standard error of minimal MSE for our predictions.

```{r}
y_hat <- predict(LASSO_fit, newx=predictor_matrix, s=lambda_optimal_one_SE) %>% 
  as.vector()
```


### Rescale predictors (Work in progress)

```{r}
normalize <- function(x){(x-mean(x))/(sd(x))}
credit_rescaled <- credit %>% 
  mutate_if(is.numeric, normalize)

model_formula <- as.formula("Balance ~ Income + Limit + Rating + Student + Cards + Age + Education + Married")
predictor_matrix <- model.matrix(model_formula, data = credit_rescaled)[, -1]
lambda_inputs <- 10^seq(-2, 10, length = 100)
LASSO_fit <- glmnet(x=predictor_matrix, y=credit_rescaled$Balance, alpha = 1, lambda = lambda_inputs)
LASSO_coefficients <- get_LASSO_coefficients(LASSO_fit)

LASSO_CV <- cv.glmnet(x=predictor_matrix, y=credit_rescaled$Balance, alpha = 1, lambda = lambda_inputs)

# Optimal lambdas
lambda_optimal <- LASSO_CV$lambda.min
lambda_optimal_one_SE <- LASSO_CV$lambda.1se


plot_LASSO_coefficients <- LASSO_coefficients %>% 
  filter(term != "(Intercept)") %>% 
  ggplot(aes(x=lambda, y=estimate, col=term)) +
  geom_line() +
  scale_x_log10() +
  labs(x="lambda (log10-scale)", y="beta-hat estimate (for rescaled numerical predictors)")
plot_LASSO_coefficients +
  geom_vline(xintercept = lambda_optimal, col="red", alpha=0.4, linetype="dashed") +
  geom_vline(xintercept = lambda_optimal_one_SE, col="blue", alpha=0.4, linetype="dashed") +
  coord_cartesian(xlim=c(min(lambda_inputs), 1))
```









# Categorical outcomes ($\geq$ 2 levels)

```{r}
library(tidyverse)
iris <- iris %>%
  as_tibble() %>%
  # Add ID column:
  mutate(ID = 1:n()) %>% 
  select(ID, Species, Sepal.Length, Sepal.Width)
```

## Classification and regression trees

### Fit/train model

```{r}
library(rpart)
model_formula <- as.formula(Species ~ Sepal.Length + Sepal.Width)
tree_parameters <- rpart.control(maxdepth = 3)
model_CART <- rpart(model_formula, data = iris, control=tree_parameters)

# Alas there is no broom functionality; this won't work
# model_CART %>% broom::tidy()
# 
# Use these instead, but they are not that helpful IMO
# print(model_CART)
# summary(model_CART)

# Plot
plot(model_CART, margin=0.25)
text(model_CART, use.n = TRUE)
title("Predicting iris species using sepal length & width")
box()
```

<!--
Where "Yes, go left. No, go right." So for example, recall our outcome variable
is a categorical variable with 3 levels: setosa, versicolor, and virginia.

```{r}
iris %>% 
  filter(Sepal.Length < 5.45, Sepal.Width < 2.8) %>% 
  count(Species)
```

and

```{r}
iris %>% 
  filter(Sepal.Length >= 5.45, Sepal.Length < 6.15, Sepal.Width < 3.1) %>% 
  count(Species)
```
-->


### Get fitted probabilities/predictions

**Output 1:** Get fitted probabilities

```{r}
p_hat_matrix <- model_CART %>% 
  predict(type = "prob", newdata = iris)

# Look at a random sample of 5 of them
p_hat_matrix %>% 
  as_tibble() %>% 
  sample_n(5)

# Score/error
MLmetrics::MultiLogLoss(y_true = iris$Species, y_pred = p_hat_matrix)
```

**Output 2:** Get explicit predictions y_hat based on fitted probabilities with
ties broken at random.

```{r, eval=FALSE}
y_hat <- model_CART %>% 
  predict(newdata=iris, type="class")

# Score/error
MLmetrics::Accuracy(y_true = iris$Species, y_pred = y_hat)
MLmetrics::ConfusionMatrix(y_true = iris$Species, y_pred = y_hat)
```
```{r, echo=FALSE}
y_hat <- model_CART %>% 
  predict(newdata=iris, type="class")

# Score/error
MLmetrics::Accuracy(y_true = iris$Species, y_pred = y_hat)
MLmetrics::ConfusionMatrix(y_true = iris$Species, y_pred = y_hat)
```



## $k$-nearest neighbors

### Fit/train model

```{r}
library(caret)
library(MLmetrics)

k <- 3
model_formula <- as.formula(Species ~ Sepal.Length + Sepal.Width)
model_knn <- caret::knn3(model_formula, data=iris, k = k)
```

### Get fitted probabilities/predictions

**Output 1:** Get fitted probabilities

```{r}
p_hat_matrix <- model_knn %>% 
  predict(newdata=iris, type="prob") %>% 
  round(3)

# Look at a random sample of 5 of them
p_hat_matrix %>% 
  as_tibble() %>% 
  sample_n(5)

# Score/error
MLmetrics::MultiLogLoss(y_true = iris$Species, y_pred = p_hat_matrix)
```

**Output 2:** Get explicit predictions y_hat based on fitted probabilities with
ties broken at random.

```{r, eval=FALSE}
y_hat <- model_knn %>% 
  predict(newdata=iris, type="class")

# Score/error
MLmetrics::Accuracy(y_true = iris$Species, y_pred = y_hat)
MLmetrics::ConfusionMatrix(y_true = iris$Species, y_pred = y_hat)
```
```{r, echo=FALSE}
y_hat <- model_knn %>% 
  predict(newdata=iris, type="class")

# Score/error
MLmetrics::Accuracy(y_true = iris$Species, y_pred = y_hat)
MLmetrics::ConfusionMatrix(y_true = iris$Species, y_pred = y_hat)
```


# Binary outcomes (2 levels)

```{r}
library(tidyverse)
library(broom)
library(okcupiddata)

profiles <- profiles %>%
  as_tibble() %>%
  # Create binary outcome variable y:
  mutate(y = ifelse(sex=="f", 1, 0)) %>%
  # Remove heights below 50 inches:
  filter(height>50) %>%
  # Add ID column:
  mutate(ID = 1:n()) %>%
  select(ID, sex, y, height) %>%
  # Remove all rows with NA missing values:
  na.omit()
profiles_train <- profiles %>% 
  sample_frac(0.5)
profiles_test <- profiles %>% 
  anti_join(profiles_train, by="ID")
```

## Logistic regression via `glm`

### Fit/train model

```{r, cache=TRUE}
model_formula <- as.formula(y~height)
model_logistic <- glm(model_formula, data=profiles_train, family="binomial")

# 1.a) Extract regression table in tidy format
model_logistic %>% 
  broom::tidy(conf.int=TRUE)

# 1.b) Extract point-by-point info in tidy format
model_logistic %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  sample_n(5)

# 1.c) Extract summary stats info in tidy format
model_logistic %>% 
  broom::glance()
```

### Predict outcomes for test data

```{r, cache=TRUE}
# 2. Make predictions on test data
# Method 1:
# -input: profiles_test is a data frame
# -output: log_odds_hat is a vector of log odds
log_odds_hat <- predict(model_logistic, newdata=profiles_test)
p_hat <- 1/(1 + exp(-log_odds_hat))

# Method 2: All new variables start with a period
model_logistic %>% 
  broom::augment(newdata=profiles_test) %>% 
  as_tibble() %>% 
  mutate(p_hat = 1/(1 + exp(-.fitted))) %>% 
  sample_n(5)
```

### Plot

```{r, cache=TRUE}
fitted_model <- model_logistic %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  mutate(p_hat = 1/(1 + exp(-.fitted)))
predictions <- model_logistic %>% 
  broom::augment(newdata=profiles_test) %>% 
  mutate(p_hat = 1/(1 + exp(-.fitted)))

# Logistic regression is fitted in log-odds(p) space
ggplot(NULL) +
  geom_line(data=fitted_model, aes(x=height, y=.fitted), col="blue") +
  geom_point(data=predictions, aes(x=height, y=.fitted), col="red") +
  labs(x="height (in inches)", y="Fitted log-odds of p_hat", title="Fitted log-odds of probability of being female vs height")

# Convert back to probability space
ggplot(NULL) +
  # Add observed binary y's, and put a little random jitter to the points
  geom_jitter(data=fitted_model, aes(x=height, y=y), height=0.05, alpha=0.05) +
  geom_line(data=fitted_model, aes(x=height, y=p_hat), col="blue") +
  geom_point(data=predictions, aes(x=height, y=p_hat), col="red") +
  labs(x="height (in inches)", y="p_hat", title="Fitted probability of being female vs height")
```

### ROC curve {#ROC_curve}

```{r}
profiles_train_augmented <- model_logistic %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  mutate(p_hat = 1/(1+exp(-.fitted)))

library(ROCR)
# This bit of code computes the ROC curve
pred <- prediction(predictions = profiles_train_augmented$p_hat, labels = profiles_train_augmented$y)
perf <- performance(pred, "tpr","fpr")

# This bit of code computes the Area Under the Curve
auc <- as.numeric(performance(pred,"auc")@y.values)
auc

# This bit of code prints it
plot(perf, main=paste("Area Under the Curve =", round(auc, 3)))
abline(c(0, 1), lty=2)
```




# Continuous outcomes

```{r}
library(tidyverse)
library(broom)

# Continuous outcome:
mtcars <- mtcars %>% 
  mutate(ID = 1:n()) %>% 
  select(ID, mpg, hp) %>% 
  as_tibble()
mtcars_train <- mtcars %>% 
  sample_frac(0.5)
mtcars_test <- mtcars %>% 
  anti_join(mtcars_train, by="ID")
```


## Regression via `lm`

### Fit/train model

```{r}
model_formula <- as.formula("mpg ~ hp")
model_lm <- lm(model_formula, data=mtcars_train)

# 1.a) Extract regression table in tidy format
model_lm %>% 
  broom::tidy(conf.int=TRUE)

# 1.b) Extract point-by-point info in tidy format
model_lm %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  sample_n(5)

# 1.c) Extract summary stats info in tidy format
model_lm %>% 
  broom::glance()
```

### Predict outcomes for test data

```{r}
# 2. Make predictions on test data
# Method 1:
# -input: mtcars_test is a data frame
# -output: y_hat is a vector
y_hat <- predict(model_lm, newdata=mtcars_test)

# Method 2: All new variables start with a period
model_lm %>% 
  broom::augment(newdata=mtcars_test) %>% 
  as_tibble() %>% 
  sample_n(5)
```

### Plot

```{r}
fitted_model <- model_lm %>% 
  broom::augment() %>% 
  as_tibble()
predictions <- model_lm %>% 
  broom::augment(newdata=mtcars_test)

ggplot(NULL) +
  geom_point(data=fitted_model, aes(x=hp, y=mpg)) +
  geom_line(data=fitted_model, aes(x=hp, y=.fitted), col="blue") +
  geom_point(data=predictions, aes(x=hp, y=.fitted), col="red") +
  labs(x="Horse power", y="Miles per gallon")
```



## LOESS {#loess}

### Fit/train model

```{r}
model_formula <- as.formula("mpg ~ hp")
model_loess <- loess(model_formula, data=mtcars_train, span=0.9)

# 1.a) Extract point-by-point info in tidy format
model_loess %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  sample_n(5)
```

### Predict outcomes for test data

```{r}
# 2. Make predictions on test data
# Method 1:
# -input: mtcars_test is a data frame
# -output: y_hat is a vector
y_hat <- predict(model_loess, newdata=mtcars_test)

# Method 2: All new variables start with a period
model_loess %>% 
  broom::augment(newdata=mtcars_test) %>% 
  sample_n(5)
```

### Plot

```{r}
fitted_model <- model_loess %>% 
  broom::augment() %>% 
  as_tibble()
predictions <- model_loess %>% 
  broom::augment(newdata=mtcars_test) %>% 
  as_tibble()

ggplot(NULL) +
  geom_point(data=fitted_model, aes(x=hp, y=mpg)) +
  geom_line(data=fitted_model, aes(x=hp, y=.fitted), col="blue") +
  geom_point(data=predictions, aes(x=hp, y=.fitted), col="red") +
  labs(x="Horse power", y="Miles per gallon")
```





## Splines {#splines}

### Fit/train model

```{r}
model_spline <- smooth.spline(x=mtcars_train$hp, y=mtcars_train$mpg, df = 4)

# 1.a) Extract point-by-point info in tidy format
model_spline %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  sample_n(5)

# 1.b) Extract summary stats info in tidy format
model_spline %>% 
  broom::glance()
```

### Predict outcomes for test data

```{r}
# 2. Make predictions on test data
# Method 1:
# -input: mtcars_test$hp is a vector
# -output: is a list with two slots: x & y
spline_fitted <- predict(model_spline, x=mtcars_test$hp)

# Convert y_hat to tibble data frame with x, y columns
spline_fitted <- spline_fitted %>% 
  as_tibble() %>% 
  rename(hp = x, .fitted = y)

y_hat <- spline_fitted$.fitted
```

### Plot

```{r}
fitted_model <- model_spline %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  rename(hp = x, mpg = y)
predictions <- mtcars_test %>% 
  mutate(.fitted = y_hat)

ggplot(NULL) +
  geom_point(data=fitted_model, aes(x=hp, y=mpg)) +
  geom_line(data=fitted_model, aes(x=hp, y=.fitted), col="blue") +
  geom_point(data=predictions, aes(x=hp, y=.fitted), col="red") +
  labs(x="Horse power", y="Miles per gallon")
```


