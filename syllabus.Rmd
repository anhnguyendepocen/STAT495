---
title: "Syllabus"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
---

<style>
h1{font-weight: 400;}
</style>

# Basic information

* **Course title:** STAT/MATH 495 - Advanced Data Analysis
* **Instructor:** [Albert Y. Kim](https://rudeboybert.rbind.io/) - Lecturer of Statistics
* **~~Email:~~ Slack team**: <a target="_blank" class="page-link" href="https://stat495-fall-2017.slack.com/">stat495-fall-2017.slack.com</a>
    + I will respond to ~~emails~~ Slack messages within 24h, but not during weekends.
    + Please only Slack message me with administrative and briefer questions as I prefer addressing more substantive questions in person.
* **Meeting locations/times:**
    + **Lectures**: M 9:00-9:50 and Tu/Th 8:30-9:50 in Merrill Science Center 131.
    + **Office hours**: M 1:00-4:00 and Th 2:00-5:00 (or by appointment) in Converse Hall 316. Late changes to office hours will be posted on Slack.
    + **Drop-in tutoring**: TBA
* **Important dates:**



***


    
# Course Description

* **Official course description**: On [Amherst College
Webpage](https://www.amherst.edu/academiclife/departments/courses/1718F/STAT/STAT-495-1718F)
* **Unofficial course description**: A course on the theoretical underpinnings
of machine learning. Fundamental concepts of machine learning, such as
crossvalidation and the bias-variance tradeoff, will be viewed through both
statistical and mathematical lenses. As much of the coursework will center
around participation in [Kaggle competitions](https://www.kaggle.com/), there
will be greater emphasis on supervised learning techniques including regression,
smoothing methods, classification, and regularization/shrinkage methods. To this
end, there will be a large computational component to the course, in particular
the use of tools for data visualization, data wrangling, and data modeling.
Furthermore, to encourage engagement with the open-source statistics, data
science, and machine learning communities, work and collaboration will center
around the use of GitHub.



***


    
# Topic Schedule

Roughly speaking we will cover the following topics. A more detailed outline can be found
[here](https://docs.google.com/spreadsheets/d/e/2PACX-1vSoT25RqvzJrr563nky-ANl_EiM39NruG2fkH8fykFxMwk8JXILo5AbdEKtJabcBQ2QM41ab4MDe7Ab/pubhtml?gid=1289440669&single=true).

1. Background
    * The humble model: $y = f(\vec{x}) + \epsilon$
    * Sampling
    * Crossvalidation
1. Continuous outcome
    * Regression for prediction
    * Splines
    * LOESS smoother
1. Bias-variance tradeoff
1. Categorical outcome AKA classification
    * Logistic regression for prediction
    * ROC curves
    * k-Nearest Neighbors
    * Classification and regression trees (CART)
1. Regularization/shrinkage methods
    * Ridge regression
    * LASSO
1. Other methods
    * Boosting and bagging
    * Random forests
    * Neural nets
1. Unsupervised learning (time permitting)
    * Principal components analysis
    * k-Means Clustering



***


    
# Materials

We will use chiefly use two freely available textbooks:

1. ["An Introduction to Statistical Learning"](http://www-bcf.usc.edu/~gareth/ISL/) by James, Witten, Hastie, and Tibshirani.
1. ["Computer Age Statistical Inference"](https://web.stanford.edu/~hastie/CASI/) by Efron and Tibshirani

Hard copies of these books will be made available on reserve in the Merrill Science Library. 



***



# Evaluation

There are four components to your final grade: problem sets, 3 midterms, engagement,
and the final project.

## 1. Weekly Problem Sets 10%

The weekly problem sets in this class should be viewed as low-stakes
opportunities to develop one's machine learning toolbox and receive feedback on
the progress of one's learning, instead of evaluative tools used by the
instructor to assign grades. To reinforce this thinking, each homework is worth
only a nominal portion of the final grade. However, not making an honest effort
on the homeworks will ultimately hurt you for your (individual) final project.

Collaboration on the homeworks is highly encouraged as in many situations 
learning is best done in groups. However you must submit your own answers and
not simple rewordings of another's work. Furthermore, **all collaborations must
be explicitly acknowledged at the top of your submissions**.

* Assigned/due on Tuesdays.
* Lowest two scores dropped.
* No email submissions will be accepted; ask a classmate to print it for you.
* No extensions for any problem sets will be granted. 

## 2. Engagement 10%

It is difficult to explicit codify what constitutes "an engaged student", so 
instead I present the following rough principle I will follow: **you'll only get
out of this class as much as you put in**. Some examples of behavior counter to this
principle:

* Merely attending lectures and not participating in discussions.
* Leveraging previous experience in other settings to coast through this course.
* Not coming to office hours when the situation warrants it. 
* Submitting homework that has code or content that is copied from (or only
slightly modified versions of) your peers' work, going against the philosophy of
the homeworks being opportunities for practice and feedback, rather than as items 
to be graded on.


## 3. Three Midterms 45%

* Two in-class and one during finals week.
* Midterm dates: TBA
* All midterms are cumulative.
* There is no extra-credit work to improve midterm scores after the fact.
* There will be no make-up nor rescheduled midterms, except in the following
cases if documentation is provided (e.g. a dean's note):
    + serious illness or death in the family.
    + athletic commitments, religious obligations, and job interviews if
    prior notice is given. In such cases, rescheduled exams must be taken
    *before* the rest of the class.


## 4. Group Final Project 35%

Much of this course is a build up to the final project, which is a capstone 
experience synthesizing everything you've learned over the course of the 
semester. You will be participating in a Kaggle competition.



***


    
# Academic Accommodations for Disabilities

